{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the 13 CSVs of CEO Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os   #C:\\Users\\Bloody Dachi\\Documents\\CS_401\\Final_Project\\CEO-Topic-Classifier-Extractor\\Finished_Tweet_CSV\n",
    "path =r'C:/Users/Bloody Dachi/Documents/CS_401/Final_Project/CEO-Topic-Classifier-Extractor/Finished_Tweet_CSV' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles: \n",
    "    df = pd.read_csv(file_,index_col=None, encoding = \"latin1\",lineterminator='\\n')\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>CEO_Full_Name</th>\n",
       "      <th>CEO_User_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@pjournel The bag itself is from dsptch in San...</td>\n",
       "      <td>Phil Libin</td>\n",
       "      <td>plibin\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I MacGyvered extra storage pouches to my backp...</td>\n",
       "      <td>Phil Libin</td>\n",
       "      <td>plibin\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Snack times Nuts equals Snuts, people. Itâs ...</td>\n",
       "      <td>Phil Libin</td>\n",
       "      <td>plibin\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Looking forward to being back in Beijing this ...</td>\n",
       "      <td>Phil Libin</td>\n",
       "      <td>plibin\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>For better and for worse, but mostly for the b...</td>\n",
       "      <td>Phil Libin</td>\n",
       "      <td>plibin\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Tweet  \\\n",
       "0           0  @pjournel The bag itself is from dsptch in San...   \n",
       "1           1  I MacGyvered extra storage pouches to my backp...   \n",
       "2           2  Snack times Nuts equals Snuts, people. Itâs ...   \n",
       "3           3  Looking forward to being back in Beijing this ...   \n",
       "4           4  For better and for worse, but mostly for the b...   \n",
       "\n",
       "  CEO_Full_Name CEO_User_Name\\r  \n",
       "0    Phil Libin        plibin\\r  \n",
       "1    Phil Libin        plibin\\r  \n",
       "2    Phil Libin        plibin\\r  \n",
       "3    Phil Libin        plibin\\r  \n",
       "4    Phil Libin        plibin\\r  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame = frame.drop(['Unnamed: 0'], axis=1)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "903277"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Multi-Index to reference Tweets of specific CEOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = [frame[\"CEO_Full_Name\"]]\n",
    "index = pd.MultiIndex.from_arrays(arrays, names=['CEO_Names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEO_Names</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Phil Libin</th>\n",
       "      <td>@pjournel The bag itself is from dsptch in San...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil Libin</th>\n",
       "      <td>I MacGyvered extra storage pouches to my backp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil Libin</th>\n",
       "      <td>Snack times Nuts equals Snuts, people. Itâs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil Libin</th>\n",
       "      <td>Looking forward to being back in Beijing this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phil Libin</th>\n",
       "      <td>For better and for worse, but mostly for the b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       Tweets\n",
       "CEO_Names                                                    \n",
       "Phil Libin  @pjournel The bag itself is from dsptch in San...\n",
       "Phil Libin  I MacGyvered extra storage pouches to my backp...\n",
       "Phil Libin  Snack times Nuts equals Snuts, people. Itâs ...\n",
       "Phil Libin  Looking forward to being back in Beijing this ...\n",
       "Phil Libin  For better and for worse, but mostly for the b..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(list(frame[\"Tweet\"]), index=index, columns = [\"Tweets\"])\n",
    "(df_new).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Time to rebalance? RT @AccountingEdit: Manuel: When it comes to the economy â\\x80\\x93 buckle your seatbelts! #dcpa18',\n",
       "       'Agree! RT @mmaycpa: What a treat to have @AnjaManuel1 presenting at #DCPA18. Very informative presentation about tech, &amp; global players China, India.',\n",
       "       'RT @AccountingEdit: Manuel: If thereâ\\x80\\x99s an AI race between the U.S. and China, itâ\\x80\\x99s not at all clear whoâ\\x80\\x99s going to come out on top. #dcpa18',\n",
       "       ...,\n",
       "       'RT @maragoni: #FutureReady is for everyone. Non-financial #blockchain uses will touch everything from real estate to app development to diamond industry: @ronqman #DCPA17',\n",
       "       'Listening to @rmp289 of @ChristensenInst about how to think about disruption #dcpa17 https://t.co/XMO4WGHCT1',\n",
       "       'He was great and he is coming to Maryland next Weds to open our CPA Summit - https://t.co/VrZsHNI5Uv live and by webcast. #futureready https://t.co/Rs1pSciR5o'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc['Tom Hood']['Tweets'].values#.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nbcnews', 'this', 'is', 'completely', 'backwards', 'based', 'on', 'what', 'we', 've', 'learned', 'from', 'the', 'hawthorne', 'test', 'tunnel', 'we', 're', 'moving', 'forward', 'with', 'a', 'much', 'larger', 'tunnel', 'network', 'under', 'la', 'won', 't', 'need', 'a', 'second', 'test', 'tunnel', 'under', 'sepulveda'], ['you', 'can', 'summon', 'your', 'tesla', 'from', 'your', 'phone', 'only', 'short', 'distances', 'today', 'but', 'in', 'a', 'few', 'years', 'summon', 'will', 'work', 'from', 'across', 'the', 'continent', 'https', 't', 'co', 'xcj67ajz8h'], ['cool', 'actually', 'if', 'you', 'buy', 'a', 'tesla', 'without', 'a', 'test', 'drive', 'you', 'have', '3', 'days', 'to', 'return', 'it', 'if', 'you', 'buy', 'after', 'a', 'test', 'drive', 'you', 'still', 'have', '24', 'hours', 'trying', 'to', 'incent', 'buying', 'with', 'no', 'test', 'drive', 'https', 't', 'co', 'o2dd5bgxrz']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_words = []\n",
    "for i in df_new.loc['Elon Musk']['Tweets'].values:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    token_words.append(tokens)\n",
    "    \n",
    "    #tokens = tokenizer.tokenize(token_words)\n",
    "print(token_words[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmzatizing and Stemming of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    texts = [p_stemmer.stem(i) for i in stop_free]\n",
    "    punc_free = ''.join(ch for ch in texts if ch not in exclude)\n",
    "    \n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in df_new.loc['Elon Musk']['Tweets'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nbcnews', 'completely', 'backwards', 'based', 'we\\x92ve', 'learned', 'hawthorne', 'test', 'tunnel', 'we\\x92re', 'moving', 'forward', 'much', 'larger', 'tunnel', 'network', 'la', 'won\\x92t', 'need', 'second', 'test', 'tunnel', 'sepulveda'], ['summon', 'tesla', 'phone', 'short', 'distance', 'today', 'year', 'summon', 'work', 'across', 'continent', 'httpstcoxcj67ajz8h'], ['cool', 'actually', 'buy', 'tesla', 'without', 'test', 'drive', '3', 'day', 'return', 'it', 'buy', 'test', 'drive', 'still', '24', 'hour', 'trying', 'incent', 'buying', 'test', 'drive', 'httpstcoo2dd5bgxrz']]\n"
     ]
    }
   ],
   "source": [
    "print(doc_clean[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1791\n"
     ]
    }
   ],
   "source": [
    "print(len(df_new.loc['Elon Musk']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# KEEP\n",
    "\n",
    "# tweetDictionary = {}\n",
    "# for i in df_new.index.unique():\n",
    "#     tweetDictionary[i[0]] = df_new.loc[i[0]]['Tweets'].values\n",
    "# for x in tweetDictionary:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_matrix(dic):\n",
    "    docs = dic\n",
    "    n = len(docs)\n",
    "    tf = {}\n",
    "    df = {}\n",
    "    length = {}\n",
    "    vocab = set()\n",
    "    for doc in docs:#list of names\n",
    "        tweets = docs[doc]\n",
    "        for i in range(len(tweets)):\n",
    "            if type(tweets[i]) == str:\n",
    "                words = nltk.word_tokenize(tweets[i])\n",
    "                words = [p_stemmer.stem(i) for i in words if i not in stop] #remove stopwords\n",
    "                words = [ch for ch in words if ch not in exclude]\n",
    "                length[doc] = 0\n",
    "                for word in words:\n",
    "                    if not word.isalpha():\n",
    "                        continue\n",
    "                    length[doc] += 1\n",
    "                    word = word.lower()\n",
    "                    vocab.add(word)\n",
    "                    if (word, doc) in tf:\n",
    "                        tf[word, doc] += 1\n",
    "                    else:\n",
    "                        tf[word, doc] = 1\n",
    "                    if word in df:\n",
    "                        df[word].add(doc)\n",
    "                    else:\n",
    "                        df[word] = set([doc])\n",
    "\n",
    "    tf_idf = {}\n",
    "    for word, doc in tf:\n",
    "        tf_idf[word, doc] = (tf[word, doc] / length[doc]) * math.log(n / len(df[word]), 10)\n",
    "        #tf_idf[word, doc] = (1 + math.log(tf[word, doc], 10)) * math.log(n / len(df[word]), 10)\n",
    "    return tf_idf, vocab, docs.keys()\n",
    "\n",
    "def cos(v1, v2):\n",
    "    if numpy.linalg.norm(v1) == 0 or numpy.linalg.norm(v2) == 0:\n",
    "        return 0\n",
    "    return numpy.dot(v1, v2) / (numpy.linalg.norm(v1) * numpy.linalg.norm(v2))\n",
    "\n",
    "\n",
    "def getVector(word, tf_idf, docs):\n",
    "    n = len(docs)\n",
    "    v = numpy.zeros(n)\n",
    "\n",
    "    for i in range(n):\n",
    "        doc = docs[i]\n",
    "        if (word, doc) in tf_idf:\n",
    "            v[i] = tf_idf[word, doc]\n",
    "\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf, vocab, docs = tf_idf_matrix(tweetDictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting specific CEO to compare tweets against list of other CEOs. This will find the 30 closest CEOs who post similar tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CEOs most similar to ''Elon Musk'':\n",
      "   Lauren Cook  0.970864\n",
      "   Don Tapscott 0.964673\n",
      "   Phil Libin   0.964533\n",
      "   Sundar Pichai 0.963710\n",
      "   Tom Hood     0.963100\n",
      "   Joni Thomas Doolin 0.961163\n",
      "   Jason Byrne  0.941851\n",
      "   evÅk ad agency 0.941810\n",
      "   RobLane      0.941674\n",
      "   Richard Jalichandra 0.941648\n",
      "   Bob Pritchett 0.941481\n",
      "   Peter Bordes 0.941423\n",
      "   Dwight Gibbs 0.941391\n",
      "   Jason Keath  0.941360\n",
      "   Helen Todd   0.941146\n",
      "   scotwingo    0.941138\n",
      "   Jason Kintzler 0.941130\n",
      "   Kira Wampler 0.941104\n",
      "   Ramit Sethi  0.941079\n",
      "   Kathy Calvin 0.940925\n",
      "   Rachel Levy  0.940899\n",
      "   kelkelly     0.940862\n",
      "   Jacquelyn Cyr 0.940862\n",
      "   Brian Armstrong 0.940809\n",
      "   Steve Goldstein 0.940762\n",
      "   Michael Hyatt 0.940756\n",
      "   Joel Dehlin  0.940750\n",
      "   Kristina Halvorson 0.940730\n",
      "   Jeremy Toeman 0.940725\n",
      "   Cassandra Bailey 0.940701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = list(docs)\n",
    "name = \"Elon Musk\"\n",
    "tweets = tweetDictionary[name]\n",
    "total_words = []\n",
    "for i in range(len(tweets)):\n",
    "    if type(tweets[i]) == str:\n",
    "        words = nltk.word_tokenize(tweets[i])\n",
    "        words = [p_stemmer.stem(i) for i in words if i not in stop] #remove stopwords\n",
    "        words = [ch for ch in words if ch not in exclude]\n",
    "        for word in words:\n",
    "            if word not in total_words:\n",
    "                total_words.append(word)\n",
    "sum_words = len(total_words)\n",
    "sum_w_vec = numpy.zeros(len(docs))\n",
    "for word in total_words:\n",
    "    wordVector = getVector(word, tf_idf, docs)\n",
    "    sum_w_vec = wordVector + sum_w_vec\n",
    "doc_Cen = sum_w_vec/sum_words\n",
    "\n",
    "cosines = {}\n",
    "total_words = []\n",
    "for doc in docs:#list of names\n",
    "    tweets = tweetDictionary[doc]\n",
    "    for i in range(len(tweets)):\n",
    "        if type(tweets[i]) == str:\n",
    "            words = nltk.word_tokenize(tweets[i])\n",
    "            words = [p_stemmer.stem(i) for i in words if i not in stop] #remove stopwords\n",
    "            words = [ch for ch in words if ch not in exclude]\n",
    "            for word in words:\n",
    "                if word not in total_words:\n",
    "                    total_words.append(word)\n",
    "    word_sum = len(total_words)\n",
    "    doc_sum = numpy.zeros(len(docs))\n",
    "    for WORD in total_words:\n",
    "        v = getVector(WORD, tf_idf, docs)\n",
    "        doc_sum = doc_sum + v\n",
    "\n",
    "        centroid = doc_sum/word_sum\n",
    "        cosines[doc] = cos(doc_Cen, centroid)\n",
    "\n",
    "c2 = [(cosines[word], word) for word in cosines]\n",
    "c2.sort()\n",
    "print('\\nCEOs most similar to ' + \"''\" + name + \"''\"+ ':')\n",
    "for c, w in reversed(c2[-30:]):\n",
    "    print('   {0:<12} {1:<.6f}'.format(w, c)) #prints out 30 most similar CEO's to Elon Musk\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cosine_value = []\n",
    "Similar_CEO_Names = []\n",
    "for c, w in reversed(c2[-30:]):\n",
    "    cosine_value.append(c)\n",
    "    Similar_CEO_Names.append(w)\n",
    "\n",
    "cosine_value=pd.Series(cosine_value)\n",
    "Similar_CEO_Names=pd.Series(Similar_CEO_Names)\n",
    "list_similar_CEOs = pd.DataFrame()\n",
    "list_similar_CEOs[\"cosine_value\"] = cosine_value\n",
    "list_similar_CEOs[\"Similar_CEO_Names\"] = Similar_CEO_Names\n",
    "list_similar_CEOs\n",
    "user_name_list = []\n",
    "#a = [frame[\"CEO_User_Name\"] for x in list_similar_CEOs if x in frame]\n",
    "#print(frame.columns)\n",
    "for x in list_similar_CEOs[\"Similar_CEO_Names\"]:\n",
    "    for j in range(len(frame[\"CEO_Full_Name\"])):\n",
    "        if x == frame[\"CEO_Full_Name\"][j]:\n",
    "            if frame[\"CEO_User_Name\\r\"][j] not in user_name_list:\n",
    "                user_name_list.append(frame[\"CEO_User_Name\\r\"][j])\n",
    "user_name_list=pd.Series(user_name_list)\n",
    "list_similar_CEOs[\"Similar_CEO_User_Names\"] = user_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_similar_CEOs.to_csv(\"Similar_CEOs_Names_List.csv\", sep = \"\\t\",encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cosine_value</th>\n",
       "      <th>Similar_CEO_Names</th>\n",
       "      <th>Similar_CEO_User_Names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.970864</td>\n",
       "      <td>Lauren Cook</td>\n",
       "      <td>laurencook\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.964673</td>\n",
       "      <td>Don Tapscott</td>\n",
       "      <td>dtapscott\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.964533</td>\n",
       "      <td>Phil Libin</td>\n",
       "      <td>plibin\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.963710</td>\n",
       "      <td>Sundar Pichai</td>\n",
       "      <td>sundarpichai\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.963100</td>\n",
       "      <td>Tom Hood</td>\n",
       "      <td>tomhood\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cosine_value Similar_CEO_Names Similar_CEO_User_Names\n",
       "0      0.970864       Lauren Cook           laurencook\\r\n",
       "1      0.964673      Don Tapscott            dtapscott\\r\n",
       "2      0.964533        Phil Libin               plibin\\r\n",
       "3      0.963710     Sundar Pichai         sundarpichai\\r\n",
       "4      0.963100          Tom Hood              tomhood\\r"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_similar_CEOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenght = len(list_similar_CEOs[\"cosine_value\"].values)\n",
    "X_train = list_similar_CEOs[\"cosine_value\"][:lenght//2]\n",
    "X_test = list_similar_CEOs[\"cosine_value\"][lenght//2:]\n",
    "y_train = list_similar_CEOs[\"Similar_CEO_Names\"][:lenght//2]\n",
    "y_test = list_similar_CEOs[\"cosine_value\"][lenght//2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x1 = np.array(X_train.values.tolist())\n",
    "x2 = np.array(X_test.values.tolist())\n",
    "y1 = np.array(y_train.tolist())\n",
    "y2 = np.array(y_test.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# train classifier\n",
    "clf = SVC(probability=True, kernel='rbf')\n",
    "clf.fit(x1.reshape(-1, 1), y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667,\n",
       "       0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667,\n",
       "       0.06666667, 0.06666667, 0.06666667, 0.06666667, 0.06666667])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = clf.predict_proba(x2.reshape(-1, 1))\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
