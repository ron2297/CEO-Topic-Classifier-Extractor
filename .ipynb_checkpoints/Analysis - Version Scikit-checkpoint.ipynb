{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining the 13 CSVs of CEO Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os   \n",
    "path =r'C:/Users/Bloody Dachi/Documents/CS_401/Final_Project/Collected_Tweets_Grouped_By_50' # use your path\n",
    "allFiles = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "list_ = []\n",
    "\n",
    "for file_ in allFiles: \n",
    "    df = pd.read_csv(file_,index_col=None, encoding = \"latin1\",lineterminator='\\n')\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_, axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>CEO_Full_Name</th>\n",
       "      <th>CEO_User_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The #HardTrend of the new tax law #TCJA is an ...</td>\n",
       "      <td>Tom Hood</td>\n",
       "      <td>tomhood\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Today is the day! Don't miss the #FreeCPE sess...</td>\n",
       "      <td>Tom Hood</td>\n",
       "      <td>tomhood\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Looking forward to another great #DCPA18 in Ma...</td>\n",
       "      <td>Tom Hood</td>\n",
       "      <td>tomhood\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"data analytics and artificial intelligence ar...</td>\n",
       "      <td>Tom Hood</td>\n",
       "      <td>tomhood\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Join me and IMA CEO @IMA_JeffThomson in a conv...</td>\n",
       "      <td>Tom Hood</td>\n",
       "      <td>tomhood\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Tweet  \\\n",
       "0           0  The #HardTrend of the new tax law #TCJA is an ...   \n",
       "1           1  Today is the day! Don't miss the #FreeCPE sess...   \n",
       "2           2  Looking forward to another great #DCPA18 in Ma...   \n",
       "3           3  \"data analytics and artificial intelligence ar...   \n",
       "4           4  Join me and IMA CEO @IMA_JeffThomson in a conv...   \n",
       "\n",
       "  CEO_Full_Name CEO_User_Name\\r  \n",
       "0      Tom Hood       tomhood\\r  \n",
       "1      Tom Hood       tomhood\\r  \n",
       "2      Tom Hood       tomhood\\r  \n",
       "3      Tom Hood       tomhood\\r  \n",
       "4      Tom Hood       tomhood\\r  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#frame = frame.drop(['Unnamed: 0'], axis=1)\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "901608"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Multi-Index to reference Tweets of specific CEOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrays = [frame[\"CEO_Full_Name\"]]\n",
    "index = pd.MultiIndex.from_arrays(arrays, names=['CEO_Names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CEO_Names</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Tom Hood</th>\n",
       "      <td>The #HardTrend of the new tax law #TCJA is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom Hood</th>\n",
       "      <td>Today is the day! Don't miss the #FreeCPE sess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom Hood</th>\n",
       "      <td>Looking forward to another great #DCPA18 in Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom Hood</th>\n",
       "      <td>\"data analytics and artificial intelligence ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tom Hood</th>\n",
       "      <td>Join me and IMA CEO @IMA_JeffThomson in a conv...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      Tweets\n",
       "CEO_Names                                                   \n",
       "Tom Hood   The #HardTrend of the new tax law #TCJA is an ...\n",
       "Tom Hood   Today is the day! Don't miss the #FreeCPE sess...\n",
       "Tom Hood   Looking forward to another great #DCPA18 in Ma...\n",
       "Tom Hood   \"data analytics and artificial intelligence ar...\n",
       "Tom Hood   Join me and IMA CEO @IMA_JeffThomson in a conv..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(list(frame[\"Tweet\"]), index=index, columns = [\"Tweets\"])\n",
    "(df_new).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The #HardTrend of the new tax law #TCJA is an opportunity review your pricing and value to clients. What is it worth to help them navigate the complexity and constant changing tax laws? #Tax https://t.co/z6qeorrx0R',\n",
       "       \"Today is the day! Don't miss the #FreeCPE session on Management Accounting in the Digital Age - Relevance Gained today 1:00pm - 2:30pm (eastern) IMA-MACPA-BLI #FutureReady https://t.co/VHLh8OlOIR\",\n",
       "       'Looking forward to another great #DCPA18 in Maryland!!!! #innovation #inspiration #technology https://t.co/RBJgiP83xs',\n",
       "       ...,\n",
       "       '창\\x80\\x98Nobody knows anything창\\x80\\x99 - William Goldman think Blair Witch. Works in innovation and business same way @mbrandolph #dcpa17',\n",
       "       'RT @JoeyHavensCPA: #DCPA17 @mbrandolph the disruptor coming after you, doesn창\\x80\\x99t look like you, is willing to do things you don창\\x80\\x99t want to do. Avoid disruption by disrupting yourself. #beEvenBetter',\n",
       "       'To innovate 1) You need a tolerance for risk 2) You need an idea @mbrandolph #dcpa17'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.loc['Tom Hood']['Tweets'].values#.head()\n",
    "#Add[\"Tweets\"] in order to access the tweets of "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nbcnews', 'this', 'is', 'completely', 'backwards', 'based', 'on', 'what', 'we', 've', 'learned', 'from', 'the', 'hawthorne', 'test', 'tunnel', 'we', 're', 'moving', 'forward', 'with', 'a', 'much', 'larger', 'tunnel', 'network', 'under', 'la', 'won', 't', 'need', 'a', 'second', 'test', 'tunnel', 'under', 'sepulveda'], ['you', 'can', 'summon', 'your', 'tesla', 'from', 'your', 'phone', 'only', 'short', 'distances', 'today', 'but', 'in', 'a', 'few', 'years', 'summon', 'will', 'work', 'from', 'across', 'the', 'continent', 'https', 't', 'co', 'xcj67ajz8h'], ['cool', 'actually', 'if', 'you', 'buy', 'a', 'tesla', 'without', 'a', 'test', 'drive', 'you', 'have', '3', 'days', 'to', 'return', 'it', 'if', 'you', 'buy', 'after', 'a', 'test', 'drive', 'you', 'still', 'have', '24', 'hours', 'trying', 'to', 'incent', 'buying', 'with', 'no', 'test', 'drive', 'https', 't', 'co', 'o2dd5bgxrz']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "token_words = []\n",
    "for i in df_new.loc['Elon Musk']['Tweets'].values:\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    token_words.append(tokens)\n",
    "    \n",
    "    #tokens = tokenizer.tokenize(token_words)\n",
    "print(token_words[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmzatizing and Stemming of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    texts = [p_stemmer.stem(i) for i in stop_free]\n",
    "    punc_free = ''.join(ch for ch in texts if ch not in exclude)\n",
    "    \n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in df_new.loc['Elon Musk']['Tweets'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nbcnews', 'completely', 'backwards', 'based', 'we\\x92ve', 'learned', 'hawthorne', 'test', 'tunnel', 'we\\x92re', 'moving', 'forward', 'much', 'larger', 'tunnel', 'network', 'la', 'won\\x92t', 'need', 'second', 'test', 'tunnel', 'sepulveda'], ['summon', 'tesla', 'phone', 'short', 'distance', 'today', 'year', 'summon', 'work', 'across', 'continent', 'httpstcoxcj67ajz8h'], ['cool', 'actually', 'buy', 'tesla', 'without', 'test', 'drive', '3', 'day', 'return', 'it', 'buy', 'test', 'drive', 'still', '24', 'hour', 'trying', 'incent', 'buying', 'test', 'drive', 'httpstcoo2dd5bgxrz']]\n"
     ]
    }
   ],
   "source": [
    "print(doc_clean[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nbcnews',\n",
       " 'completely',\n",
       " 'backwards',\n",
       " 'based',\n",
       " 'we\\x92ve',\n",
       " 'learned',\n",
       " 'hawthorne',\n",
       " 'test',\n",
       " 'tunnel',\n",
       " 'we\\x92re',\n",
       " 'moving',\n",
       " 'forward',\n",
       " 'much',\n",
       " 'larger',\n",
       " 'tunnel',\n",
       " 'network',\n",
       " 'la',\n",
       " 'won\\x92t',\n",
       " 'need',\n",
       " 'second',\n",
       " 'test',\n",
       " 'tunnel',\n",
       " 'sepulveda']"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_clean[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import math\n",
    "import os\n",
    "\n",
    "def tf_idf_matrix(path):\n",
    "    docs = path\n",
    "    n = len(path)\n",
    "    tf = {}\n",
    "    df = {}\n",
    "    length = {}\n",
    "    vocab = set()\n",
    "    for doc in range(len(docs)):\n",
    "        length[doc] = 0\n",
    "        for word in path[doc]:\n",
    "            length[doc] +=1\n",
    "            vocab.add(word)\n",
    "            if(word,doc) in tf:\n",
    "                tf[word,doc] += 1\n",
    "            else:\n",
    "                tf[word,doc] = 1\n",
    "            if word in path[doc]:\n",
    "                df[word].add(doc)\n",
    "            else:\n",
    "                df[word] = set([doc])\n",
    "\n",
    "    tf_idf = {}\n",
    "    for word, doc in tf:\n",
    "        tf_idf[word, doc] = (tf[word, doc] / length[doc]) * math.log(n / len(df[word]), 10)\n",
    "        #tf_idf[word, doc] = (1 + math.log(tf[word, doc], 10)) * math.log(n / len(df[word]), 10)\n",
    "\n",
    "    return tf_idf, vocab, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'nbcnews'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-207-3fc2d064555d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-206-debe5502f165>\u001b[0m in \u001b[0;36mtf_idf_matrix\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mtf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m                 \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'nbcnews'"
     ]
    }
   ],
   "source": [
    "tf_idf_matrix(doc_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=9, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "n_features = 50\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=50,\n",
    "                                stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(stuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actually', 'air', 'amp', 'best', 'better', 'car', 'company', 'day', 'dmcryan', 'don', 'drive', 'exactly', 'fredericlambert', 'going', 'good', 'great', 'high', 'know', 'like', 'love', 'make', 'maybe', 'medium', 'model', 'month', 'need', 'new', 'people', 'point', 'production', 'really', 'right', 'soon', 'spacex', 'team', 'tesla', 'thanks', 'thing', 'think', 'time', 'true', 'uaw', 'use', 'want', 'way', 'week', 'work', 'yeah', 'year', 'yes']\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print(tf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_components=9, n_jobs=1, n_topics=None, perp_tol=0.1,\n",
       "             random_state=0, topic_word_prior=None,\n",
       "             total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "amp work actually good high drive know make maybe year\n",
      "Topic #1:\n",
      "model spacex great love maybe production drive time air right\n",
      "Topic #2:\n",
      "tesla month drive exactly uaw true good make amp way\n",
      "Topic #3:\n",
      "production better exactly true way day point high air don\n",
      "Topic #4:\n",
      "good people fredericlambert think air year time thanks medium work\n",
      "Topic #5:\n",
      "like make time need really use going month production want\n",
      "Topic #6:\n",
      "yes year dmcryan team soon best fredericlambert going actually drive\n",
      "Topic #7:\n",
      "car don know right yeah want tesla uaw production drive\n",
      "Topic #8:\n",
      "company thanks medium uaw thing week new drive work maybe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_top_words(lda, tf_feature_names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}